{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b14baad",
   "metadata": {},
   "source": [
    "# Social Media Disaster Detection using NLP\n",
    "\n",
    "**Internship-ready notebook**\n",
    "\n",
    "This notebook contains a complete pipeline (data loading, preprocessing, feature extraction, model training & evaluation) to detect disaster-related social media posts. Save your dataset as `disaster_tweets.csv` in the same folder as the notebook or update the path in the data-loading cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas scikit-learn nltk imbalanced-learn matplotlib seaborn\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import pickle\n",
    "\n",
    "# For text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources (first run)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "print('Setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "# Place your CSV in the same folder and name it 'disaster_tweets.csv' or change the path below.\n",
    "DATA_PATH = 'disaster_tweets.csv'\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f'Loaded {len(df)} rows from {DATA_PATH}')\n",
    "else:\n",
    "    # Fallback: create a small example dataset\n",
    "    data = {\n",
    "        'text': [\n",
    "            'Massive earthquake shakes the city, buildings collapsed, people trapped',\n",
    "            'Lovely day at the park with family! #weekend',\n",
    "            'Flooding reported in downtown area, residents advised to evacuate',\n",
    "            'New cafe opened near me, great coffee and vibes ☕️',\n",
    "            'Wildfire spreading fast due to strong winds, emergency declared'\n",
    "        ],\n",
    "        'target': [1, 0, 1, 0, 1]  # 1 = disaster-related, 0 = not disaster\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print('No dataset found — using a small example dataset')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5584564",
   "metadata": {},
   "source": [
    "## Quick EDA\n",
    "Check class distribution and example posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fab4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "if 'target' in df.columns:\n",
    "    print(df['target'].value_counts())\n",
    "else:\n",
    "    print('Column `target` not found. If your dataset has labels under a different column name, rename it to `target` or update this cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18c751",
   "metadata": {},
   "source": [
    "## Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing utilities\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # lower, remove urls, mentions, hashtags (keep hashtag text), emojis, non-alphanumeric\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    text = re.sub(r'#', ' ', text)  # remove hash symbol but keep the word\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "if 'text' in df.columns:\n",
    "    df['clean_text'] = df['text'].astype(str).apply(preprocess_text)\n",
    "    df[['text','clean_text']].head()\n",
    "else:\n",
    "    raise KeyError('Dataset must contain a `text` column with the social media content.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f607db",
   "metadata": {},
   "source": [
    "## Handle class imbalance (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dataset is imbalanced, you can upsample the minority class (simple approach)\n",
    "if 'target' in df.columns:\n",
    "    display(df['target'].value_counts())\n",
    "    # Simple check\n",
    "    class_counts = df['target'].value_counts()\n",
    "    if class_counts.min() / class_counts.max() < 0.6:\n",
    "        # Upsample minority class\n",
    "        majority = df[df['target'] == class_counts.idxmax()]\n",
    "        minority = df[df['target'] != class_counts.idxmax()]\n",
    "        minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
    "        df_bal = pd.concat([majority, minority_upsampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        print('Performed upsampling. New distribution:')\n",
    "        display(df_bal['target'].value_counts())\n",
    "    else:\n",
    "        df_bal = df.copy()\n",
    "else:\n",
    "    df_bal = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca63d04e",
   "metadata": {},
   "source": [
    "## Train / Test split and model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X = df_bal['clean_text']\n",
    "y = df_bal['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define a simple pipeline: TF-IDF + Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1,2))),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6782e3",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce07dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and evaluation\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('\\nConfusion Matrix:\\n', cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ad874",
   "metadata": {},
   "source": [
    "## Try alternative model: Random Forest (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c6a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1,2))),\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "print('RF Accuracy:', accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed08bc4",
   "metadata": {},
   "source": [
    "## Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256872d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best pipeline (change filename as needed)\n",
    "model_path = 'disaster_detector_pipeline.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "print(f'Model saved to {model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d5a3d",
   "metadata": {},
   "source": [
    "## Example: Use the saved model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and predict on new samples\n",
    "with open('disaster_detector_pipeline.pkl','rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "examples = [\n",
    "    'Huge landslide reported near the highway, many cars buried under debris',\n",
    "    'Enjoying the concert tonight! Amazing performance.'\n",
    "]\n",
    "examples_clean = [preprocess_text(t) for t in examples]\n",
    "preds = model.predict(examples_clean)\n",
    "for txt, p in zip(examples, preds):\n",
    "    print('\\nText:', txt)\n",
    "    print('Predicted label (1=disaster,0=not):', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ad2de",
   "metadata": {},
   "source": [
    "## Optional: Collecting live tweets (Twitter API)\n",
    "\n",
    "Below is a **commented** template showing how you would collect tweets via Tweepy. To run it you must set up API credentials and install tweepy. This is left commented because it requires private keys and network access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (commented) - requires `tweepy` and Twitter API credentials\n",
    "# !pip install tweepy\n",
    "# import tweepy\n",
    "#\n",
    "# API_KEY = 'YOUR_API_KEY'\n",
    "# API_SECRET = 'YOUR_API_SECRET'\n",
    "# BEARER_TOKEN = 'YOUR_BEARER_TOKEN'\n",
    "#\n",
    "# client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "# query = 'earthquake OR flood OR fire -is:retweet lang:en'\n",
    "# tweets = client.search_recent_tweets(query=query, max_results=100)\n",
    "# for t in tweets.data:\n",
    "#     print(t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655d837",
   "metadata": {},
   "source": [
    "## Next steps / Improvements\n",
    "\n",
    "- Use transformer embeddings (BERT, RoBERTa) for better performance.\n",
    "- Fine-tune a pretrained transformer on a labeled disaster dataset.\n",
    "- Add location extraction and entity recognition to find affected areas.\n",
    "- Build a dashboard to visualize detected events (time series, heatmaps).\n",
    "- Add multilingual support and misinformation filtering.\n",
    "\n",
    "---\n",
    "\n",
    "Good luck with your internship project!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
